#!/usr/bin/env python3
"""
AST (Audio Spectrogram Transformer) éŸ³éŸ¿ã‚¤ãƒ™ãƒ³ãƒˆæ¤œå‡ºAPI - Supabaseçµ±åˆç‰ˆ
file_pathsãƒ™ãƒ¼ã‚¹ã®å‡¦ç†ã§audio_filesãƒ†ãƒ¼ãƒ–ãƒ«ã¨é€£æº
"""

import os
import io
import json
import tempfile
import traceback
from typing import List, Dict, Optional
from datetime import datetime
import time

import torch
import numpy as np
import librosa
import soundfile as sf
from transformers import AutoFeatureExtractor, ASTForAudioClassification
from fastapi import FastAPI, HTTPException
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import uvicorn

# AWS S3ã¨Supabase
import boto3
from botocore.exceptions import ClientError
from supabase import create_client, Client
from dotenv import load_dotenv

# ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã¿
load_dotenv()

# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã§ãƒ¢ãƒ‡ãƒ«ã‚’ä¿æŒ
model = None
feature_extractor = None
id2label = None

# ãƒ¢ãƒ‡ãƒ«å
MODEL_NAME = "MIT/ast-finetuned-audioset-10-10-0.4593"

# Supabaseã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–
supabase_url = os.getenv('SUPABASE_URL')
supabase_key = os.getenv('SUPABASE_KEY')

if not supabase_url or not supabase_key:
    raise ValueError("SUPABASE_URLãŠã‚ˆã³SUPABASE_KEYãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")

supabase: Client = create_client(supabase_url, supabase_key)
print(f"âœ… Supabaseæ¥ç¶šè¨­å®šå®Œäº†: {supabase_url}")

# AWS S3ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–
aws_access_key_id = os.getenv('AWS_ACCESS_KEY_ID')
aws_secret_access_key = os.getenv('AWS_SECRET_ACCESS_KEY')
s3_bucket_name = os.getenv('S3_BUCKET_NAME', 'watchme-vault')
aws_region = os.getenv('AWS_REGION', 'us-east-1')

if not aws_access_key_id or not aws_secret_access_key:
    raise ValueError("AWS_ACCESS_KEY_IDãŠã‚ˆã³AWS_SECRET_ACCESS_KEYãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")

s3_client = boto3.client(
    's3',
    aws_access_key_id=aws_access_key_id,
    aws_secret_access_key=aws_secret_access_key,
    region_name=aws_region
)
print(f"âœ… AWS S3æ¥ç¶šè¨­å®šå®Œäº†: ãƒã‚±ãƒƒãƒˆ={s3_bucket_name}, ãƒªãƒ¼ã‚¸ãƒ§ãƒ³={aws_region}")

# FastAPIã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
app = FastAPI(
    title="AST Audio Event Detection API with Supabase",
    description="Audio Spectrogram Transformer ã‚’ä½¿ç”¨ã—ãŸéŸ³éŸ¿ã‚¤ãƒ™ãƒ³ãƒˆæ¤œå‡ºAPIï¼ˆSupabaseçµ±åˆç‰ˆï¼‰",
    version="2.0.0"
)

# CORSãƒŸãƒ‰ãƒ«ã‚¦ã‚§ã‚¢ã®è¨­å®š
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«
class FetchAndProcessPathsRequest(BaseModel):
    file_paths: List[str]
    threshold: Optional[float] = 0.1
    top_k: Optional[int] = 3
    analyze_timeline: Optional[bool] = True  # ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³åˆ†æã‚’å®Ÿè¡Œã™ã‚‹ã‹
    segment_duration: Optional[float] = 10.0  # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®é•·ã•ï¼ˆç§’ï¼‰- 10ç§’ãŒæœ€é©
    overlap: Optional[float] = 0.0  # ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ç‡ - ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ãªã—ãŒæœ€é©

def load_model():
    """ãƒ¢ãƒ‡ãƒ«ã¨feature extractorã‚’èª­ã¿è¾¼ã‚€"""
    global model, feature_extractor, id2label
    
    print(f"ğŸ”„ ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­: {MODEL_NAME}")
    try:
        feature_extractor = AutoFeatureExtractor.from_pretrained(MODEL_NAME)
        model = ASTForAudioClassification.from_pretrained(MODEL_NAME)
        
        # ãƒ©ãƒ™ãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’å–å¾—
        id2label = model.config.id2label
        
        # CPUã§å®Ÿè¡Œ
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        model.to(device)
        model.eval()
        
        print(f"âœ… ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã«æˆåŠŸã—ã¾ã—ãŸ")
        print(f"   - ãƒ‡ãƒã‚¤ã‚¹: {device}")
        print(f"   - ãƒ©ãƒ™ãƒ«æ•°: {len(id2label)}")
        
    except Exception as e:
        print(f"âŒ ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}")
        raise

def extract_info_from_file_path(file_path: str) -> Dict[str, str]:
    """
    ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‹ã‚‰ãƒ‡ãƒã‚¤ã‚¹IDã€æ—¥ä»˜ã€æ™‚é–“ãƒ–ãƒ­ãƒƒã‚¯ã‚’æŠ½å‡º
    
    Args:
        file_path: S3ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ (ä¾‹: files/device-id/2025-07-20/14-30/audio.wav)
    
    Returns:
        device_id, date, time_block ã‚’å«ã‚€è¾æ›¸
    """
    parts = file_path.split('/')
    
    if len(parts) >= 5:
        device_id = parts[1]
        date = parts[2]
        time_block = parts[3]
        return {
            'device_id': device_id,
            'date': date,
            'time_block': time_block
        }
    else:
        # ãƒ‘ã‚¹ãŒæœŸå¾…ã™ã‚‹å½¢å¼ã§ãªã„å ´åˆ
        return {
            'device_id': 'unknown',
            'date': 'unknown',
            'time_block': 'unknown'
        }

async def update_audio_files_status(file_path: str, status: str = 'completed'):
    """
    audio_filesãƒ†ãƒ¼ãƒ–ãƒ«ã®behavior_features_statusã‚’æ›´æ–°ï¼ˆYamNetã¨åŒã˜ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’ä½¿ç”¨ï¼‰
    
    Args:
        file_path: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        status: ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ ('pending', 'processing', 'completed', 'error')
    """
    try:
        update_response = supabase.table('audio_files') \
            .update({'behavior_features_status': status}) \
            .eq('file_path', file_path) \
            .execute()
        
        if update_response.data:
            print(f"âœ… ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æ›´æ–°æˆåŠŸ: {file_path} -> {status}")
            return True
        else:
            print(f"âš ï¸ å¯¾è±¡ãƒ¬ã‚³ãƒ¼ãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_path}")
            return False
            
    except Exception as e:
        print(f"âŒ ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æ›´æ–°ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return False

async def save_to_behavior_yamnet(device_id: str, date: str, time_block: str, 
                                  timeline_data: List[Dict]):
    """
    behavior_yamnetãƒ†ãƒ¼ãƒ–ãƒ«ã«ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³å½¢å¼ã®çµæœã‚’ä¿å­˜
    
    Args:
        device_id: ãƒ‡ãƒã‚¤ã‚¹ID
        date: æ—¥ä»˜
        time_block: æ™‚é–“ãƒ–ãƒ­ãƒƒã‚¯
        timeline_data: ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³å½¢å¼ã®ã‚¤ãƒ™ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿
    """
    try:
        # ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³å½¢å¼ã®ãƒ‡ãƒ¼ã‚¿ã‚’eventsã‚«ãƒ©ãƒ ã«ä¿å­˜
        data = {
            'device_id': device_id,
            'date': date,
            'time_block': time_block,
            'events': timeline_data,  # ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³å½¢å¼ã®ãƒ‡ãƒ¼ã‚¿
            'status': 'completed'  # ASTã«ã‚ˆã‚‹å‡¦ç†å®Œäº†
        }
        
        # upsertï¼ˆæ—¢å­˜ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Œã°æ›´æ–°ã€ãªã‘ã‚Œã°æŒ¿å…¥ï¼‰
        response = supabase.table('behavior_yamnet') \
            .upsert(data, on_conflict='device_id,date,time_block') \
            .execute()
        
        if response.data:
            print(f"âœ… ãƒ‡ãƒ¼ã‚¿ä¿å­˜æˆåŠŸ: {device_id}/{date}/{time_block}")
            return True
        else:
            print(f"âš ï¸ ãƒ‡ãƒ¼ã‚¿ä¿å­˜å¤±æ•—: ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒç©ºã§ã™")
            return False
            
    except Exception as e:
        print(f"âŒ ãƒ‡ãƒ¼ã‚¿ä¿å­˜ã‚¨ãƒ©ãƒ¼: {str(e)}")
        traceback.print_exc()
        return False

def download_from_s3(file_path: str, local_path: str) -> bool:
    """
    S3ã‹ã‚‰éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    
    Args:
        file_path: S3ã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        local_path: ãƒ­ãƒ¼ã‚«ãƒ«ä¿å­˜å…ˆãƒ‘ã‚¹
    
    Returns:
        æˆåŠŸæ™‚Trueã€å¤±æ•—æ™‚False
    """
    try:
        print(f"ğŸ“¥ S3ã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­: {file_path}")
        s3_client.download_file(s3_bucket_name, file_path, local_path)
        print(f"âœ… ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†: {file_path}")
        return True
    except ClientError as e:
        error_code = e.response['Error']['Code']
        if error_code == '404':
            print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_path}")
        else:
            print(f"âŒ S3ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {error_code} - {str(e)}")
        return False
    except Exception as e:
        print(f"âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return False

def process_audio(audio_data: np.ndarray, sample_rate: int) -> np.ndarray:
    """
    éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’å‰å‡¦ç†ã™ã‚‹
    
    Args:
        audio_data: éŸ³å£°ãƒ‡ãƒ¼ã‚¿ï¼ˆnumpyé…åˆ—ï¼‰
        sample_rate: ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆ
    
    Returns:
        å‡¦ç†æ¸ˆã¿ã®éŸ³å£°ãƒ‡ãƒ¼ã‚¿
    """
    # ãƒ¢ãƒãƒ©ãƒ«ã«å¤‰æ›
    if len(audio_data.shape) > 1:
        audio_data = np.mean(audio_data, axis=1)
    
    # ãƒ¢ãƒ‡ãƒ«ãŒæœŸå¾…ã™ã‚‹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆã«ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆ16kHzï¼‰
    target_sr = feature_extractor.sampling_rate
    if sample_rate != target_sr:
        audio_data = librosa.resample(
            audio_data, 
            orig_sr=sample_rate, 
            target_sr=target_sr
        )
    
    # float32ã«å¤‰æ›
    if audio_data.dtype != np.float32:
        audio_data = audio_data.astype(np.float32)
    
    # æ­£è¦åŒ–ï¼ˆ-1.0 ã€œ 1.0ï¼‰
    max_val = np.max(np.abs(audio_data))
    if max_val > 0:
        audio_data = audio_data / max_val
    
    return audio_data

def predict_audio_events(audio_data: np.ndarray, top_k: int = 5, threshold: float = 0.1) -> List[Dict]:
    """
    éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰éŸ³éŸ¿ã‚¤ãƒ™ãƒ³ãƒˆã‚’äºˆæ¸¬
    
    Args:
        audio_data: å‰å‡¦ç†æ¸ˆã¿ã®éŸ³å£°ãƒ‡ãƒ¼ã‚¿
        top_k: è¿”ã™ä¸Šä½äºˆæ¸¬ã®æ•°
        threshold: æœ€å°ç¢ºç‡ã—ãã„å€¤
    
    Returns:
        äºˆæ¸¬çµæœã®ãƒªã‚¹ãƒˆ
    """
    # ç‰¹å¾´æŠ½å‡º
    inputs = feature_extractor(
        audio_data,
        sampling_rate=feature_extractor.sampling_rate,
        return_tensors="pt"
    )
    
    # ãƒ‡ãƒã‚¤ã‚¹ã«ç§»å‹•
    device = next(model.parameters()).device
    inputs = {k: v.to(device) for k, v in inputs.items()}
    
    # æ¨è«–å®Ÿè¡Œ
    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs.logits
    
    # Softmaxã§ç¢ºç‡ã«å¤‰æ›
    probs = torch.nn.functional.softmax(logits, dim=-1)[0]
    
    # ä¸Šä½kå€‹ã‚’å–å¾—
    top_probs, top_indices = torch.topk(probs, min(top_k, len(probs)))
    
    # çµæœã‚’æ•´å½¢
    predictions = []
    for prob, idx in zip(top_probs.cpu(), top_indices.cpu()):
        score = prob.item()
        if score >= threshold:  # ã—ãã„å€¤ä»¥ä¸Šã®ã¿
            label_id = idx.item()
            label = id2label.get(label_id) or id2label.get(str(label_id)) or f"Event_{label_id}"
            predictions.append({
                "label": label,
                "score": round(score, 4)
            })
    
    return predictions

def analyze_timeline(audio_data: np.ndarray, sample_rate: int, 
                    segment_duration: float = 1.0, 
                    overlap: float = 0.5,
                    top_k: int = 3,
                    threshold: float = 0.1) -> Dict:
    """
    éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’æ™‚ç³»åˆ—ã§åˆ†æ
    
    Args:
        audio_data: éŸ³å£°ãƒ‡ãƒ¼ã‚¿
        sample_rate: ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆ
        segment_duration: ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®é•·ã•ï¼ˆç§’ï¼‰
        overlap: ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ç‡ (0-1)
        top_k: å„æ™‚åˆ»ã§è¿”ã™ã‚¤ãƒ™ãƒ³ãƒˆæ•°
        threshold: æœ€å°ç¢ºç‡ã—ãã„å€¤
    
    Returns:
        æ™‚ç³»åˆ—åˆ†æçµæœ
    """
    # éŸ³å£°ã‚’å‰å‡¦ç†
    processed_audio = process_audio(audio_data, sample_rate)
    target_sr = feature_extractor.sampling_rate
    
    # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆè¨­å®š
    segment_samples = int(segment_duration * target_sr)
    hop_samples = int(segment_samples * (1 - overlap))
    
    # ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³çµæœã‚’æ ¼ç´
    timeline = []
    all_events = {}
    
    # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã”ã¨ã«å‡¦ç†
    for i in range(0, len(processed_audio) - segment_samples + 1, hop_samples):
        segment = processed_audio[i:i + segment_samples]
        time_position = i / target_sr
        
        # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®äºˆæ¸¬
        events = predict_audio_events(segment, top_k, threshold)
        
        # ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ã«è¿½åŠ 
        timeline.append({
            "time": round(time_position, 1),
            "events": events
        })
        
        # ã‚¤ãƒ™ãƒ³ãƒˆã®é›†è¨ˆ
        for event in events:
            label = event["label"]
            if label not in all_events:
                all_events[label] = {"count": 0, "total_score": 0}
            all_events[label]["count"] += 1
            all_events[label]["total_score"] += event["score"]
    
    # æœ€ã‚‚é »ç¹ãªã‚¤ãƒ™ãƒ³ãƒˆã‚’é›†è¨ˆ
    most_common = []
    for label, stats in sorted(all_events.items(), key=lambda x: x[1]["count"], reverse=True)[:5]:
        most_common.append({
            "label": label,
            "occurrences": stats["count"],
            "average_score": round(stats["total_score"] / stats["count"], 4)
        })
    
    return {
        "timeline": timeline,
        "summary": {
            "total_segments": len(timeline),
            "duration_seconds": round(len(processed_audio) / target_sr, 1),
            "segment_duration": segment_duration,
            "overlap": overlap,
            "most_common_events": most_common
        }
    }

async def process_single_file(file_path: str, threshold: float = 0.1, top_k: int = 5,
                             analyze_timeline_flag: bool = True, 
                             segment_duration: float = 1.0,
                             overlap: float = 0.5) -> Dict:
    """
    å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ï¼ˆã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³å½¢å¼ã§ä¿å­˜ï¼‰
    
    Args:
        file_path: S3ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        threshold: æœ€å°ç¢ºç‡ã—ãã„å€¤
        top_k: è¿”ã™ä¸Šä½äºˆæ¸¬ã®æ•°
        analyze_timeline_flag: ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³åˆ†æã‚’å®Ÿè¡Œã™ã‚‹ã‹ï¼ˆå¸¸ã«Trueæ¨å¥¨ï¼‰
        segment_duration: ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®é•·ã•
        overlap: ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ç‡
    
    Returns:
        å‡¦ç†çµæœ
    """
    temp_file = None
    try:
        # ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ã‚’æŠ½å‡º
        file_info = extract_info_from_file_path(file_path)
        
        # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’å‡¦ç†ä¸­ã«æ›´æ–°
        await update_audio_files_status(file_path, 'processing')
        
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã¦ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as tmp:
            temp_file = tmp.name
        
        if not download_from_s3(file_path, temp_file):
            await update_audio_files_status(file_path, 'error')
            return {"status": "error", "file_path": file_path, "error": "Download failed"}
        
        # éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€
        audio_data, sample_rate = sf.read(temp_file)
        print(f"ğŸµ éŸ³å£°ãƒ­ãƒ¼ãƒ‰å®Œäº†: {len(audio_data)/sample_rate:.2f}ç§’, {sample_rate}Hz")
        
        # ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³åˆ†æã‚’å®Ÿè¡Œï¼ˆå¿…é ˆï¼‰
        timeline_result = analyze_timeline(
            audio_data, sample_rate,
            segment_duration, overlap, top_k, threshold
        )
        
        # ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³å½¢å¼ã®ãƒ‡ãƒ¼ã‚¿ã‚’behavior_yamnetãƒ†ãƒ¼ãƒ–ãƒ«ã«ä¿å­˜
        save_success = await save_to_behavior_yamnet(
            file_info['device_id'],
            file_info['date'],
            file_info['time_block'],
            timeline_result['timeline']  # ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’ä¿å­˜
        )
        
        if save_success:
            # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’å®Œäº†ã«æ›´æ–°
            await update_audio_files_status(file_path, 'completed')
            
            return {
                "status": "success",
                "file_path": file_path,
                "device_id": file_info['device_id'],
                "date": file_info['date'],
                "time_block": file_info['time_block'],
                "timeline": timeline_result
            }
        else:
            await update_audio_files_status(file_path, 'error')
            return {"status": "error", "file_path": file_path, "error": "Save failed"}
            
    except Exception as e:
        print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼: {file_path} - {str(e)}")
        traceback.print_exc()
        await update_audio_files_status(file_path, 'error')
        return {"status": "error", "file_path": file_path, "error": str(e)}
        
    finally:
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
        if temp_file and os.path.exists(temp_file):
            os.remove(temp_file)

@app.on_event("startup")
async def startup_event():
    """ã‚µãƒ¼ãƒãƒ¼èµ·å‹•æ™‚ã«ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰"""
    load_model()

@app.get("/")
async def root():
    """ãƒ«ãƒ¼ãƒˆã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    return {
        "message": "AST Audio Event Detection API with Supabase Integration",
        "model": MODEL_NAME,
        "version": "2.0.0",
        "status": "ready" if model is not None else "not ready",
        "endpoints": {
            "/fetch-and-process-paths": "Process audio files from S3 via file paths",
            "/health": "Health check endpoint"
        }
    }

@app.get("/health")
async def health_check():
    """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    return {
        "status": "healthy" if model is not None else "unhealthy",
        "model_loaded": model is not None,
        "supabase_connected": supabase is not None,
        "s3_connected": s3_client is not None
    }

@app.post("/fetch-and-process-paths")
async def fetch_and_process_paths(request: FetchAndProcessPathsRequest):
    """
    file_pathsãƒ™ãƒ¼ã‚¹ã®éŸ³éŸ¿ã‚¤ãƒ™ãƒ³ãƒˆæ¤œå‡ºã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆï¼ˆWhisper APIãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
    
    Args:
        request: file_pathsé…åˆ—ã¨ã‚ªãƒ—ã‚·ãƒ§ãƒ³ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    
    Returns:
        å‡¦ç†çµæœã®ã‚µãƒãƒªãƒ¼ã¨è©³ç´°
    """
    # ãƒ¢ãƒ‡ãƒ«ãŒãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
    if model is None or feature_extractor is None:
        raise HTTPException(status_code=503, detail="Model not loaded")
    
    start_time = time.time()
    
    # å‡¦ç†çµæœã‚’æ ¼ç´
    processed_files = []
    error_files = []
    processed_time_blocks = set()
    
    print(f"ğŸš€ å‡¦ç†é–‹å§‹: {len(request.file_paths)}å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«")
    
    # å„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†
    for file_path in request.file_paths:
        result = await process_single_file(
            file_path,
            request.threshold,
            request.top_k,
            request.analyze_timeline,
            request.segment_duration,
            request.overlap
        )
        
        if result["status"] == "success":
            processed_files.append(file_path)
            processed_time_blocks.add(result["time_block"])
        else:
            error_files.append({
                "file_path": file_path,
                "error": result.get("error", "Unknown error")
            })
    
    # å®Ÿè¡Œæ™‚é–“ã‚’è¨ˆç®—
    execution_time = time.time() - start_time
    
    # ã‚µãƒãƒªãƒ¼ã‚’ä½œæˆ
    total_files = len(request.file_paths)
    success_count = len(processed_files)
    error_count = len(error_files)
    
    response = {
        "status": "success" if error_count == 0 else "partial",
        "summary": {
            "total_files": total_files,
            "processed": success_count,
            "errors": error_count
        },
        "processed_files": processed_files,
        "processed_time_blocks": list(processed_time_blocks),
        "error_files": error_files if error_files else None,
        "execution_time_seconds": round(execution_time, 1),
        "message": f"{total_files}ä»¶ä¸­{success_count}ä»¶ã‚’æ­£å¸¸ã«å‡¦ç†ã—ã¾ã—ãŸ"
    }
    
    print(f"âœ… å‡¦ç†å®Œäº†: {success_count}/{total_files}ä»¶æˆåŠŸ (å®Ÿè¡Œæ™‚é–“: {execution_time:.1f}ç§’)")
    
    return JSONResponse(content=response)

if __name__ == "__main__":
    # ã‚µãƒ¼ãƒãƒ¼ã‚’èµ·å‹•ï¼ˆãƒãƒ¼ãƒˆ8018ã§å‹•ä½œï¼‰
    print("=" * 50)
    print("AST Audio Event Detection API with Supabase")
    print(f"Model: {MODEL_NAME}")
    print("=" * 50)
    
    uvicorn.run(
        app,
        host="127.0.0.1",
        port=8018,
        log_level="info"
    )